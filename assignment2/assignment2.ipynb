{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455f8adf",
   "metadata": {
    "id": "455f8adf"
   },
   "source": [
    "# Assignment 2: Deep Q Learning and Policy Gradient\n",
    "\n",
    "-----\n",
    "\n",
    "*CS260R 2023Fall: Reinforcement Learning. Department of Computer Science at University of California, Los Angeles.\n",
    "Course Instructor: Professor Bolei ZHOU. Assignment author: Zhenghao PENG, Yiran WANG.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc5685",
   "metadata": {
    "id": "5ecc5685"
   },
   "source": [
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| TYPE_YOUR_NAME_HERE | TYPE_YOUR_STUDENT_ID_HERE |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e080e",
   "metadata": {
    "id": "b55e080e"
   },
   "source": [
    "Welecome to the assignment 2 of our RL course. This assignment consisits of three parts:\n",
    "\n",
    "* Section 2: Implement Q learning in tabular setting (20 points)\n",
    "* Section 3: Implement Deep Q Network with pytorch (30 points)\n",
    "* Section 4: Implement policy gradient method REINFORCE with pytorch (30 points)\n",
    "* Section 5: Implement policy gradient method with baseline (20 points) (+20 points bonus)\n",
    "\n",
    "Section 0 and Section 1 set up the dependencies and prepare some useful functions.\n",
    "\n",
    "The experiments we'll conduct and their expected goals:\n",
    "\n",
    "1. Naive Q learning in FrozenLake &emsp; (should solve)\n",
    "2. DQN in CartPole &emsp; (should solve)\n",
    "3. DQN in MetaDrive-Easy &emsp; (should solve)\n",
    "4. Policy Gradient w/o baseline in CartPole (w/ and w/o advantage normalization) &emsp; (should solve)\n",
    "5. Policy Gradient w/o baseline in MetaDrive-Easy &emsp; (should solve)\n",
    "6. Policy Gradient w/ baseline in CartPole (w/ advantage normalization) &emsp; (should solve)\n",
    "7. Policy Gradient w/ baseline in MetaDrive-Easy &emsp; (should solve)\n",
    "8. Policy Gradient w/ baseline in MetaDrive-Hard &emsp; (>20 return) (Optional, +20 points bonus can be earned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0171e",
   "metadata": {},
   "source": [
    "> NOTE: MetaDrive does not support python=3.12. If you are in python=3.12, we suggest to recreate a new conda environment:\n",
    "\n",
    "```bash\n",
    "conda env remove -n cs260r\n",
    "conda create -n cs260r python=3.11 -y\n",
    "pip install notebook  # Install jupyter notebook\n",
    "jupyter notebook  # Run jupyter notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feea3d2",
   "metadata": {
    "id": "3feea3d2"
   },
   "source": [
    "## Section 0: Dependencies\n",
    "\n",
    "Please install the following dependencies.\n",
    "\n",
    "\n",
    "### Notes on MetaDrive\n",
    "\n",
    "MetaDrive is a lightweight driving simulator which we will use for DQN and Policy Gradient methods. It can not be run on M1-chip Mac. We suggest using Colab or Linux for running MetaDrive.\n",
    "\n",
    "Please ignore this warning from MetaDrive: `WARNING:root:BaseEngine is not launched, fail to sync seed to engine!`\n",
    "\n",
    "### Notes on Colab\n",
    "\n",
    "We have several cells used for installing dependencies for Colab only. Please make sure they are run properly.\n",
    "\n",
    "You don't need to install python packages again and again after **restarting the runtime**, since the Colab instance still remembers the python envionment after you installing packages for the first time. But you do need to rerun those packages installation script after you **reconnecting to the runtime** (which means Google assigns a new machine to you and thus the python environment is new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b170cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_IN_COLAB = 'google.colab' in str(get_ipython())  # Detect if it is running in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098b4a3-1f4e-4ffd-b6cb-dbec4ede65f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similar to AS1\n",
    "\n",
    "!pip install -U pip\n",
    "!pip install numpy scipy \"gymnasium<0.29\"\n",
    "!pip install torch torchvision\n",
    "!pip install mediapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05b5d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b05b5d4",
    "outputId": "07c15710-629c-419b-e3b0-1330420bfea8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install MetaDrive, a lightweight driving simulator\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.version_info.minor >= 12:\n",
    "    raise ValueError(\"MetaDrive only supports python<3.12.0.\")\n",
    "\n",
    "!pip install \"git+https://github.com/metadriverse/metadrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IlCJY9seS7RB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlCJY9seS7RB",
    "outputId": "3504c227-56ea-4db5-8abe-cea176aefcc2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test whether MetaDrive is properly installed. No error means the test is passed.\n",
    "!python -m metadrive.examples.profile_metadrive --num-steps 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612bfe5",
   "metadata": {
    "id": "9612bfe5"
   },
   "source": [
    "## Section 1: Building abstract class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064ac72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9064ac72",
    "outputId": "4eed79d8-0126-4a00-ff90-869135e2d144"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Import some packages that we need to use\n",
    "import mediapy as media\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gymnasium.error import Error\n",
    "from gymnasium import logger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "import time\n",
    "import pygame\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='[%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def wait(sleep=0.2):\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(sleep)\n",
    "\n",
    "\n",
    "def merge_config(new_config, old_config):\n",
    "    \"\"\"Merge the user-defined config with default config\"\"\"\n",
    "    config = copy.deepcopy(old_config)\n",
    "    if new_config is not None:\n",
    "        config.update(new_config)\n",
    "    return config\n",
    "\n",
    "\n",
    "def test_random_policy(policy, env):\n",
    "    _acts = set()\n",
    "    for i in range(1000):\n",
    "        act = policy(0)\n",
    "        _acts.add(act)\n",
    "        assert env.action_space.contains(act), \"Out of the bound!\"\n",
    "    if len(_acts) != 1:\n",
    "        print(\n",
    "            \"[HINT] Though we call self.policy 'random policy', \" \\\n",
    "            \"we find that generating action randomly at the beginning \" \\\n",
    "            \"and then fixing it during updating values period lead to better \" \\\n",
    "            \"performance. Using purely random policy is not even work! \" \\\n",
    "            \"We encourage you to investigate this issue.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# We register a non-slippery version of FrozenLake environment.\n",
    "try:\n",
    "    gym.register(\n",
    "        id='FrozenLakeNotSlippery-v1',\n",
    "        entry_point='gymnasium.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=200,\n",
    "        reward_threshold=0.78,  # optimum = .8196\n",
    "    )\n",
    "except Error:\n",
    "    print(\"The environment is registered already.\")\n",
    "\n",
    "\n",
    "def _render_helper(env, sleep=0.1):\n",
    "    ret = env.render()\n",
    "    if sleep:\n",
    "        wait(sleep=sleep)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def animate(img_array, fps=None):\n",
    "    \"\"\"A function that can generate GIF file and show in Notebook.\"\"\"\n",
    "    media.show_video(img_array, fps=fps)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes=1, seed=0, env_name='FrozenLake8x8-v1',\n",
    "             render=None, existing_env=None, max_episode_length=1000,\n",
    "             sleep=0.0, verbose=False):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    if existing_env is None:\n",
    "        render_mode = render if render else None\n",
    "        env = gym.make(env_name, render_mode=render)\n",
    "    else:\n",
    "        env = existing_env\n",
    "    try:\n",
    "        rewards = []\n",
    "        frames = []\n",
    "        succ_rate = []\n",
    "        if render:\n",
    "            num_episodes = 1\n",
    "        for i in range(num_episodes):\n",
    "            obs, info = env.reset(seed=seed + i)\n",
    "            act = policy(obs)\n",
    "            ep_reward = 0\n",
    "            for step_count in range(max_episode_length):\n",
    "                obs, reward, terminated, truncated, info = env.step(act)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                act = policy(obs)\n",
    "                ep_reward += reward\n",
    "\n",
    "                if verbose and step_count % 50 == 0:\n",
    "                    print(\"Evaluating {}/{} episodes. We are in {}/{} steps. Current episode reward: {:.3f}\".format(\n",
    "                        i + 1, num_episodes, step_count + 1, max_episode_length, ep_reward\n",
    "                    ))\n",
    "\n",
    "                if render == \"ansi\":\n",
    "                    print(_render_helper(env, sleep))\n",
    "                elif render:\n",
    "                    frames.append(_render_helper(env, sleep))\n",
    "                if done:\n",
    "                    break\n",
    "            rewards.append(ep_reward)\n",
    "            if \"arrive_dest\" in info:\n",
    "                succ_rate.append(float(info[\"arrive_dest\"]))\n",
    "        if render:\n",
    "            env.close()\n",
    "    except Exception as e:\n",
    "        env.close()\n",
    "        raise e\n",
    "    finally:\n",
    "        env.close()\n",
    "    eval_dict = {\"frames\": frames}\n",
    "    if succ_rate:\n",
    "        eval_dict[\"success_rate\"] = sum(succ_rate) / len(succ_rate)\n",
    "    return np.mean(rewards), eval_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c8c84",
   "metadata": {
    "id": "d21c8c84"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "DEFAULT_CONFIG = dict(\n",
    "    seed=0,\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "    learning_rate=0.001,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v1'\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, DEFAULT_CONFIG)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env_name = self.config['env_name']\n",
    "        self.env = gym.make(self.env_name)\n",
    "\n",
    "        # Apply the random seed\n",
    "        self.seed = self.config[\"seed\"]\n",
    "        np.random.seed(self.seed)\n",
    "        self.env.reset(seed=self.seed)\n",
    "\n",
    "        # We set self.obs_dim to the number of possible observation\n",
    "        # if observation space is discrete, otherwise the number\n",
    "        # of observation's dimensions. The same to self.act_dim.\n",
    "        if isinstance(self.env.observation_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.observation_space.shape) == 1\n",
    "            self.obs_dim = self.env.observation_space.shape[0]\n",
    "            self.discrete_obs = False\n",
    "        elif isinstance(self.env.observation_space,\n",
    "                        gym.spaces.discrete.Discrete):\n",
    "            self.obs_dim = self.env.observation_space.n\n",
    "            self.discrete_obs = True\n",
    "        else:\n",
    "            raise ValueError(\"Wrong observation space!\")\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.action_space.shape) == 1\n",
    "            self.act_dim = self.env.action_space.shape[0]\n",
    "        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\n",
    "            self.act_dim = self.env.action_space.n\n",
    "        else:\n",
    "            raise ValueError(\"Wrong action space! {}\".format(self.env.action_space))\n",
    "\n",
    "        self.eps = self.config['eps']\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"\n",
    "        Process the raw observation. For example, we can use this function to\n",
    "        convert the input state (integer) to a one-hot vector.\n",
    "        \"\"\"\n",
    "        return state\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the processed state.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_action() function.\")\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        if \"MetaDrive\" in self.env_name:\n",
    "            kwargs[\"existing_env\"] = self.env\n",
    "        result, eval_infos = evaluate(self.policy, num_episodes, seed=self.seed,\n",
    "                                      env_name=self.env_name, *args, **kwargs)\n",
    "        return result, eval_infos\n",
    "\n",
    "    def policy(self, raw_state, eps=0.0):\n",
    "        \"\"\"A wrapper function takes raw_state as input and output action.\"\"\"\n",
    "        return self.compute_action(self.process_state(raw_state), eps=eps)\n",
    "\n",
    "    def train(self, iteration=None):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf324d71",
   "metadata": {
    "id": "cf324d71"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    total_steps = 0\n",
    "\n",
    "    try:\n",
    "        for i in range(config['max_iteration'] + 1):\n",
    "            stat = trainer.train(iteration=i)\n",
    "            stat = stat or {}\n",
    "            stats.append(stat)\n",
    "            if \"episode_len\" in stat:\n",
    "                total_steps += stat[\"episode_len\"]\n",
    "            if i % config['evaluate_interval'] == 0 or \\\n",
    "                    i == config[\"max_iteration\"]:\n",
    "                reward, _ = trainer.evaluate(\n",
    "                    config.get(\"evaluate_num_episodes\", 50),\n",
    "                    max_episode_length=config.get(\"max_episode_length\", 1000)\n",
    "                )\n",
    "                logger.info(\"Iter {}, {}episodic return is {:.2f}. {}\".format(\n",
    "                    i,\n",
    "                    \"\" if total_steps == 0 else \"Step {}, \".format(total_steps),\n",
    "                    reward,\n",
    "                    {k: round(np.mean(v), 4) for k, v in stat.items()\n",
    "                     if not np.isnan(v) and k != \"frames\"\n",
    "                     }\n",
    "                    if stat else \"\"\n",
    "                ))\n",
    "                now = time.time()\n",
    "            if reward_threshold is not None and reward > reward_threshold:\n",
    "                logger.info(\"Iter {}, episodic return {:.3f} is \"\n",
    "                            \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                            \"exit the training process.\".format(i, reward, reward_threshold))\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"Error happens during training: \")\n",
    "        raise e\n",
    "    finally:\n",
    "        if hasattr(trainer.env, \"close\"):\n",
    "            trainer.env.close()\n",
    "            print(\"Environment is closed.\")\n",
    "\n",
    "    return trainer, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a565ab",
   "metadata": {
    "id": "64a565ab"
   },
   "source": [
    "## Section 2: Q-Learning\n",
    "(20/100 points)\n",
    "\n",
    "Q-learning is an off-policy algorithm who differs from SARSA in the computing of TD error. \n",
    "\n",
    "Unlike getting the TD error by running policy to get `next_act` $a'$ and compute:\n",
    "\n",
    "$r + \\gamma Q(s', a') - Q(s, a)$\n",
    "\n",
    "as in SARSA, in Q-learning we compute the TD error via:\n",
    "\n",
    "$r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$. \n",
    "\n",
    "The reason we call it \"off-policy\" is that the next-Q value is not computed for the \"behavior policy\", instead, it is a \"virtural policy\" that always takes the best action given current Q values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188997dd",
   "metadata": {},
   "source": [
    "### Section 2.1: Building Q Learning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ec18b",
   "metadata": {
    "id": "6a2ec18b"
   },
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "Q_LEARNING_TRAINER_CONFIG = merge_config(dict(\n",
    "    eps=0.3,\n",
    "), DEFAULT_CONFIG)\n",
    "\n",
    "\n",
    "class QLearningTrainer(AbstractTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = merge_config(config, Q_LEARNING_TRAINER_CONFIG)\n",
    "        super(QLearningTrainer, self).__init__(config=config)\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.eps = self.config[\"eps\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.act_dim))\n",
    "\n",
    "    def compute_action(self, obs, eps=None):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        # TODO: You need to implement the epsilon-greedy policy here.\n",
    "        pass\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self, iteration=None):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        obs, info = self.env.reset()\n",
    "        for t in range(self.max_episode_length):\n",
    "            act = self.compute_action(obs)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = self.env.step(act)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # TODO: compute the TD error, based on the next observation\n",
    "            td_error = None\n",
    "            pass\n",
    "\n",
    "            # TODO: compute the new Q value\n",
    "            # hint: use TD error, self.learning_rate and old Q value\n",
    "            new_value = None\n",
    "            pass\n",
    "\n",
    "            self.table[obs][act] = new_value\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f6c0f2",
   "metadata": {},
   "source": [
    "### Section 2.2: Use Q Learning to train agent in FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61586e08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61586e08",
    "outputId": "05ad92b5-a19b-4812-ee1a-a3c775799b3d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer, _ = run(\n",
    "    trainer_cls=QLearningTrainer,\n",
    "    config=dict(\n",
    "        max_iteration=5000,\n",
    "        evaluate_interval=50,\n",
    "        evaluate_num_episodes=50,\n",
    "        env_name='FrozenLakeNotSlippery-v1'\n",
    "    ),\n",
    "    reward_threshold=0.99\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe03c52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fe03c52",
    "outputId": "57d058af-741d-422b-d84c-26f5bbc61820"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Render the learned behavior\n",
    "_, eval_info = evaluate(\n",
    "    policy=q_learning_trainer.policy,\n",
    "    num_episodes=1,\n",
    "    env_name=q_learning_trainer.env_name,\n",
    "    render=\"rgb_array\",  # Visualize the behavior here in the cell \n",
    "    sleep=0.2  # The time interval between two rendering frames\n",
    ")\n",
    "animate(eval_info[\"frames\"], fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198cc0a",
   "metadata": {
    "id": "e198cc0a"
   },
   "source": [
    "## Section 3: Implement Deep Q Learning in Pytorch\n",
    "\n",
    "(30 / 100 points)\n",
    "\n",
    "In this section, we will implement a neural network and train it with Deep Q Learning with Pytorch, a powerful deep learning framework. \n",
    "\n",
    "If you are not familiar with Pytorch, we suggest you to go through pytorch official quickstart tutorials:\n",
    "1. [quickstart](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "2. [tutorial on RL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "\n",
    "Different from the Q learning in Section 2, we will implement Deep Q Network (DQN) in this section. The main differences are summarized as follows:\n",
    "\n",
    "**DQN requires an experience replay memory to store the transitions.** A replay memory is implemented in the following `ExperienceReplayMemory` class. It contains a certain amount of transitions: `(s_t, a_t, r_t, s_t+1, done_t)`. When the memory is full, the earliest transition is discarded and the latest one is stored.\n",
    "\n",
    "The replay memory increases the sample efficiency (since each transition might be used multiple times) when solving complex task. However, you may find it learn slowly in this assignment since the CartPole-v1 is a relatively easy environment.\n",
    "\n",
    "\n",
    "**DQN has a delayed-updating target network.** DQN maintains another neural network called the target network that has identical structure of the Q network. After a certain amount of steps has been taken, the target network copies the parameters of the Q network to itself. The update of the target network will be much less frequent than the update of the Q network, since the Q network is updated in each step.\n",
    "\n",
    "The target network is used to stabilize the estimation of the TD error. In DQN, the TD error is estimated as:\n",
    "\n",
    "$$(r_t + \\gamma \\max_{a_{t+1}} Q^{target}(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$\n",
    "\n",
    "The Q value of the next state is estimated by the target network, not the Q network that is being updated. This mechanism can reduce the variance of gradient because the next Q values is not influenced by the update of current Q network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa025029",
   "metadata": {},
   "source": [
    "### Section 3.1: Build DQN trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87cab7",
   "metadata": {
    "id": "6a87cab7"
   },
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements. When adding new element into the deque will make deque full with \n",
    "        # `maxlen` elements, the oldest element (the index 0 element) will be removed.\n",
    "\n",
    "        # TODO: uncomment next line. \n",
    "        # self.memory = deque(maxlen=capacity)\n",
    "        pass\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72ec0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a72ec0f",
    "outputId": "20a2f35b-6a96-48f1-8626-707411112d68"
   },
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_units=100):\n",
    "        super(PytorchModel, self).__init__()\n",
    "\n",
    "        # TODO: Build a nn.Sequential object as the neural network with two hidden layers and one output layer.\n",
    "        #\n",
    "        # The first hidden layer takes `num_inputs`-dim vector as input and has `hidden_units` hidden units,\n",
    "        # followed by a ReLU activation function.\n",
    "        # \n",
    "        # The second hidden layer takes `hidden_units`-dim vector as input and has `hidden_units` hidden units,\n",
    "        # followed by a ReLU activation function.\n",
    "        # \n",
    "        # The output layer takes `hidden_units`-dim vector as input and return `num_outputs`-dim vctor as output.\n",
    "        self.action_value = None\n",
    "        pass\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.action_value(obs)\n",
    "\n",
    "\n",
    "# Test\n",
    "test_pytorch_model = PytorchModel(num_inputs=3, num_outputs=7, hidden_units=123)\n",
    "assert isinstance(test_pytorch_model.action_value, nn.Module)\n",
    "assert len(test_pytorch_model.state_dict()) == 6\n",
    "assert test_pytorch_model.state_dict()[\"action_value.0.weight\"].shape == (123, 3)\n",
    "print(\"Name of each parameter vectors: \", test_pytorch_model.state_dict().keys())\n",
    "\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbd300",
   "metadata": {
    "id": "dddbd300"
   },
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "DQN_CONFIG = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    hidden_dim=100,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True,\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1,\n",
    "    env_name=\"CartPole-v1\",\n",
    "), Q_LEARNING_TRAINER_CONFIG)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQNTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, DQN_CONFIG)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "\n",
    "        # You need to setup the parameter for your function approximator.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # TODO: Initialize the Q network and the target network using PytorchModel class.\n",
    "        self.network = None\n",
    "        print(\"Setting up self.network with obs dim: {} and action dim: {}\".format(self.obs_dim, self.act_dim))\n",
    "        pass\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        # Initialize target network to be identical to self.network. \n",
    "        # You should put the weights of self.network into self.target_network.\n",
    "        # TODO: Uncomment next few lines\n",
    "        # self.target_network = PytorchModel(self.obs_dim, self.act_dim)\n",
    "        # self.target_network.load_state_dict(self.network.state_dict())\n",
    "        pass\n",
    "\n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # TODO: Uncomment next few lines\n",
    "        # self.optimizer = torch.optim.Adam(\n",
    "        #     self.network.parameters(), lr=self.learning_rate\n",
    "        # )\n",
    "        # self.loss = nn.MSELoss()\n",
    "        pass\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        values = self.network(processed_state).detach().numpy()\n",
    "        return values\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        if np.random.uniform(0, 1) < eps:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(values)\n",
    "        return action\n",
    "\n",
    "    def train(self, iteration=None):\n",
    "        iteration_string = \"\" if iteration is None else f\"Iter {iteration}: \"\n",
    "        obs, info = self.env.reset()\n",
    "        processed_obs = self.process_state(obs)\n",
    "        act = self.compute_action(processed_obs)\n",
    "\n",
    "        stat = {\"loss\": [], \"success_rate\": np.nan}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_obs, reward, terminated, truncated, info = self.env.step(act)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_processed_obs = self.process_state(next_obs)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_obs, act, reward, next_processed_obs, done)\n",
    "            )\n",
    "\n",
    "            processed_obs = next_processed_obs\n",
    "            act = self.compute_action(next_processed_obs)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                if \"arrive_dest\" in info:\n",
    "                    stat[\"success_rate\"] = info[\"arrive_dest\"]\n",
    "                break\n",
    "\n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update policy in each environmental interaction.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                logging.info(\n",
    "                    \"{}Current memory contains {} transitions, \"\n",
    "                    \"start learning!\".format(iteration_string, self.learn_start)\n",
    "                )\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of elements in transitions into tensors.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # TODO: Compute the Q values for the next states.\n",
    "                Q_t_plus_one: torch.Tensor = None\n",
    "                pass\n",
    "\n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "\n",
    "                # TODO: Compute the target value of Q.\n",
    "                Q_target = None\n",
    "                pass\n",
    "\n",
    "                assert Q_target.shape == (self.batch_size,)\n",
    "\n",
    "            self.network.train()  # Set the network to \"train\" mode.\n",
    "\n",
    "            # TODO: Collect the Q values in batch.\n",
    "            # Hint: The network will return the Q values for all actions at a given state.\n",
    "            #  So we need to \"extract\" the Q value for the action we've taken.\n",
    "            #  You need to use torch.gather to manipuate the 2nd dimension of the return\n",
    "            #  tensor from the network and extract the desired Q values.\n",
    "            Q_t: torch.Tensor = None\n",
    "            pass\n",
    "\n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            stat['loss'].append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # TODO: Apply gradient clipping with pytorch utility. Uncomment next line.\n",
    "            # nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
    "            pass\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "            self.step_since_update = 0\n",
    "\n",
    "            # TODO: Copy the weights of self.network to self.target_network.\n",
    "            pass\n",
    "\n",
    "            self.target_network.eval()\n",
    "\n",
    "        ret = {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "        if \"success_rate\" in stat:\n",
    "            ret[\"success_rate\"] = stat[\"success_rate\"]\n",
    "        return ret\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)\n",
    "\n",
    "    def save(self, loc=\"model.pt\"):\n",
    "        torch.save(self.network.state_dict(), loc)\n",
    "\n",
    "    def load(self, loc=\"model.pt\"):\n",
    "        self.network.load_state_dict(torch.load(loc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6fabd",
   "metadata": {},
   "source": [
    "### Section 3.2: Test DQN trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d4a27d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30d4a27d",
    "outputId": "7a3c068c-a580-498a-953e-275da6def868"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = DQNTrainer({})\n",
    "\n",
    "# Test compute_values\n",
    "fake_state = test_trainer.env.observation_space.sample()\n",
    "processed_state = test_trainer.process_state(fake_state)\n",
    "assert processed_state.shape == (test_trainer.obs_dim,), processed_state.shape\n",
    "values = test_trainer.compute_values(processed_state)\n",
    "assert values.shape == (test_trainer.act_dim,), values.shape\n",
    "\n",
    "test_trainer.train()\n",
    "print(\"Now your codes should be bug-free.\")\n",
    "\n",
    "_ = run(DQNTrainer, dict(\n",
    "    max_iteration=20,\n",
    "    evaluate_interval=10,\n",
    "    learn_start=100,\n",
    "    env_name=\"CartPole-v1\",\n",
    "))\n",
    "\n",
    "test_trainer.save(\"test_trainer.pt\")\n",
    "test_trainer.load(\"test_trainer.pt\")\n",
    "\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff767ce5",
   "metadata": {},
   "source": [
    "### Section 3.3: Train DQN agents in CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53bf178",
   "metadata": {},
   "source": [
    "First, we visualize a random agent in CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "eval_reward, eval_info = evaluate(\n",
    "    policy=lambda x: np.random.randint(2),\n",
    "    num_episodes=1,\n",
    "    env_name=\"CartPole-v1\",\n",
    "    render=\"rgb_array\",  # Visualize the behavior here in the cell \n",
    ")\n",
    "\n",
    "animate(eval_info[\"frames\"])\n",
    "\n",
    "print(\"A random agent achieves {} return.\".format(eval_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a5dbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "957a5dbc",
    "outputId": "71a980c5-2bad-4727-f703-a4562af7028a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, dict(\n",
    "    max_iteration=5000,\n",
    "    evaluate_interval=100,\n",
    "    learning_rate=0.001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.1,\n",
    "    target_update_freq=2000,\n",
    "    batch_size=128,\n",
    "    learn_freq=32,\n",
    "    env_name=\"CartPole-v1\",\n",
    "), reward_threshold=450.0)\n",
    "\n",
    "reward, _ = pytorch_trainer.evaluate()\n",
    "assert reward > 400.0, \"Check your codes. \" \\\n",
    "                       \"Your agent should achieve {} reward in 5000 iterations.\" \\\n",
    "                       \"But it achieve {} reward in evaluation.\".format(400.0, reward)\n",
    "\n",
    "pytorch_trainer.save(\"dqn_trainer_cartpole.pt\")\n",
    "\n",
    "# Should solve the task in 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4cf9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "66a4cf9a",
    "outputId": "ac906699-4cd4-40ad-eaea-d41b975d6a14"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Render the learned behavior\n",
    "eval_reward, eval_info = evaluate(\n",
    "    policy=pytorch_trainer.policy,\n",
    "    num_episodes=1,\n",
    "    env_name=pytorch_trainer.env_name,\n",
    "    render=\"rgb_array\",  # Visualize the behavior here in the cell \n",
    ")\n",
    "\n",
    "animate(eval_info[\"frames\"])\n",
    "\n",
    "print(\"DQN agent achieves {} return.\".format(eval_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b60f6c",
   "metadata": {},
   "source": [
    "### Section 3.4: Train DQN agents in MetaDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c9cea",
   "metadata": {
    "id": "e93c9cea"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def register_metadrive():\n",
    "    try:\n",
    "        from metadrive.envs import MetaDriveEnv\n",
    "        from metadrive.utils.config import merge_config_with_unknown_keys\n",
    "    except ImportError as e:\n",
    "        print(\"Please install MetaDrive through: pip install git+https://github.com/decisionforce/metadrive\")\n",
    "        raise e\n",
    "\n",
    "    env_names = []\n",
    "    try:\n",
    "        class MetaDriveEnvTut(gym.Wrapper):\n",
    "            def __init__(self, config, *args, render_mode=None, **kwargs):\n",
    "                # Ignore render_mode\n",
    "                self._render_mode = render_mode\n",
    "                super().__init__(MetaDriveEnv(config))\n",
    "                self.action_space = gym.spaces.Discrete(int(np.prod(self.env.action_space.n)))\n",
    "\n",
    "            def reset(self, *args, seed=None, render_mode=None, options=None, **kwargs):\n",
    "                # Ignore seed and render_mode\n",
    "                return self.env.reset(*args, **kwargs)\n",
    "\n",
    "            def render(self):\n",
    "                return self.env.render(mode=self._render_mode)\n",
    "\n",
    "        def _make_env(*args, **kwargs):\n",
    "            return MetaDriveEnvTut(*args, **kwargs)\n",
    "\n",
    "        env_name = \"MetaDrive-Tut-Easy-v0\"\n",
    "        gym.register(id=env_name, entry_point=_make_env, kwargs={\"config\": dict(\n",
    "            map=\"S\",\n",
    "            start_seed=0,\n",
    "            num_scenarios=1,\n",
    "            horizon=200,\n",
    "            discrete_action=True,\n",
    "            discrete_steering_dim=3,\n",
    "            discrete_throttle_dim=3\n",
    "        )})\n",
    "        env_names.append(env_name)\n",
    "\n",
    "        env_name = \"MetaDrive-Tut-Hard-v0\"\n",
    "        gym.register(id=env_name, entry_point=_make_env, kwargs={\"config\": dict(\n",
    "            map=\"CCC\",\n",
    "            start_seed=0,\n",
    "            num_scenarios=10,\n",
    "            discrete_action=True,\n",
    "            discrete_steering_dim=5,\n",
    "            discrete_throttle_dim=5\n",
    "        )})\n",
    "        env_names.append(env_name)\n",
    "    except gym.error.Error as e:\n",
    "        print(\"Information when registering MetaDrive: \", e)\n",
    "    else:\n",
    "        print(\"Successfully registered MetaDrive environments: \", env_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb5d1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89fb5d1e",
    "outputId": "2eb2cf53-138d-4143-b36c-eadbcfa11be7"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "register_metadrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580ec99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b580ec99",
    "outputId": "08cd25dc-2ed5-46b5-e2ed-3ecf86e61942"
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = DQNTrainer(dict(env_name=\"MetaDrive-Tut-Easy-v0\"))\n",
    "\n",
    "# Test compute_values\n",
    "for _ in range(10):\n",
    "    fake_state = test_trainer.env.observation_space.sample()\n",
    "    processed_state = test_trainer.process_state(fake_state)\n",
    "    assert processed_state.shape == (test_trainer.obs_dim,), processed_state.shape\n",
    "    values = test_trainer.compute_values(processed_state)\n",
    "    assert values.shape == (test_trainer.act_dim,), values.shape\n",
    "\n",
    "    test_trainer.train()\n",
    "\n",
    "print(\"Now your codes should be bug-free.\")\n",
    "test_trainer.env.close()\n",
    "del test_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c9809",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "675c9809",
    "outputId": "a45d5773-fc88-464b-ad5c-10dcf94b599f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "env_name = \"MetaDrive-Tut-Easy-v0\"\n",
    "\n",
    "pytorch_trainer2, _ = run(DQNTrainer, dict(\n",
    "    max_episode_length=200,\n",
    "    max_iteration=5000,\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "    learning_rate=0.0001,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=1000000,\n",
    "    learn_start=2000,\n",
    "    eps=0.1,\n",
    "    target_update_freq=5000,\n",
    "    learn_freq=16,\n",
    "    batch_size=256,\n",
    "    env_name=env_name\n",
    "), reward_threshold=120)\n",
    "\n",
    "pytorch_trainer2.save(\"dqn_trainer_metadrive_easy.pt\")\n",
    "\n",
    "# Run this cell without modification\n",
    "\n",
    "# Render the learned behavior\n",
    "# NOTE: The learned agent is marked by green color.\n",
    "eval_reward, eval_info = evaluate(\n",
    "    policy=pytorch_trainer2.policy,\n",
    "    num_episodes=1,\n",
    "    env_name=pytorch_trainer2.env_name,\n",
    "    render=\"topdown\",  # Visualize the behaviors in top-down view\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "frames = [pygame.surfarray.array3d(f).swapaxes(0, 1) for f in eval_info[\"frames\"]]\n",
    "\n",
    "animate(frames)\n",
    "\n",
    "print(\"DQN agent achieves {} return in MetaDrive easy environment.\".format(eval_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86871e",
   "metadata": {},
   "source": [
    "## Section 4: Policy gradient methods - REINFORCE\n",
    "\n",
    "(30 / 100 points)\n",
    "\n",
    "Unlike the supervised learning, in RL the optimization objective, the episodic return, is not differentiable w.r.t. the neural network parameters. This can be solved via ***Policy Gradient***. It can be proved that policy gradient is an unbiased estimator of the gradient of the objective.\n",
    "\n",
    "Concretely, let's consider such optimization objective:\n",
    "\n",
    "$$Q = \\mathbb E_{\\text{possible trajectories}} \\sum_t r(a_t, s_t) = \\sum_{s_0, a_0,..} p(s_0, a_0, ..., s_t, a_t) r(s_0, a_0, ..., s_t, a_t) = \\sum_{\\tau} p(\\tau)r(\\tau)$$ \n",
    "\n",
    "wherein $\\sum_t r(a_t, s_t) = r(\\tau)$ is the return of trajectory $\\tau = (s_0, a_0, ...)$. We remove the discount factor for simplicity.\n",
    "Since we want to maximize Q, we can simply compute the gradient of Q w.r.t. parameter $\\theta$ (which is implictly included in $p(\\tau)$):\n",
    "\n",
    "$$\\nabla_\\theta Q = \\nabla_\\theta \\sum_{\\tau} p(\\tau)r(\\tau) = \\sum_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau)$$\n",
    "\n",
    "wherein we've applied a famous trick: $\\nabla_\\theta p(\\tau) = p(\\tau)\\cfrac{\\nabla_\\theta p(\\tau)}{p(\\tau)} = p(\\tau)\\nabla_\\theta \\log p(\\tau)$. Here the $r(\\tau)$ will be determined when $\\tau$ is determined. So it has nothing to do with the policy. We can move it out from the gradient.\n",
    "\n",
    "Introducing a log term can change the product of probabilities to sum of log probabilities. Now we can expand the log of product above to sum of log:\n",
    "\n",
    "$$p_\\theta(\\tau) = p(s_0, a_0, ...) = p(s_0) \\prod_t \\pi_\\theta (a_t|s_t) p(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "$$\\log p_\\theta (\\tau) = \\log p(s_0) + \\sum_t \\log \\pi_\\theta(a_t|s_t) + \\sum_t \\log p(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "You can find that the first and third term are not correlated to the parameter of policy $\\pi_\\theta(\\cdot)$. So when we compute $\\nabla_\\theta Q$, we find \n",
    "\n",
    "$$\\nabla_\\theta Q =\n",
    "\\sum_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau) =  \n",
    "\\sum_{\\tau} r(\\tau) p(\\tau)\\nabla_\\theta \\log p(\\tau) =\n",
    "\\sum p_\\theta(\\tau) ( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) ) r(\\tau) d\\tau$$\n",
    "\n",
    "When we sample sufficient amount of data from the environment, the above equation can be estimated via:\n",
    "\n",
    "$$\\nabla_\\theta Q =\\cfrac{1}{N}\\sum_{i=1}^N [( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) (\\sum_{t'=t} \\gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) )]$$\n",
    "\n",
    "This algorithm is called REINFORCE algorithm, which is a Monte Carlo Policy Gradient algorithm with long history. In this section, we will implement the it using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f6bf3",
   "metadata": {},
   "source": [
    "The policy network is composed by two parts: \n",
    "\n",
    "1. A basic neural network serves as the function approximator. It output raw values parameterizing the action distribution given current observation. We will reuse PytorchModel here.\n",
    "2. A distribution layer builds upon the neural network to wrap the raw logits output from neural network to a distribution and provides API for sampling action and computing log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b067b5",
   "metadata": {},
   "source": [
    "### Section 4.1: Build REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccbd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PGNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_units=128):\n",
    "        super(PGNetwork, self).__init__()\n",
    "        self.network = PytorchModel(obs_dim, act_dim, hidden_units)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        logit = self.network(obs)\n",
    "\n",
    "        # TODO: Create an object of the class \"torch.distributions.Categorical\" \n",
    "        # Then sample an action from it.\n",
    "        action = None\n",
    "        pass\n",
    "\n",
    "        return action\n",
    "\n",
    "    def log_prob(self, obs, act):\n",
    "        logits = self.network(obs)\n",
    "\n",
    "        # TODO: Create an object of the class \"torch.distributions.Categorical\" \n",
    "        # Then get the log probability of the action `act` in this distribution.\n",
    "        log_prob = None\n",
    "        pass\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "# Note that we do not implement GaussianPolicy here. So we can't\n",
    "# apply our algorithm to the environment with continous action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d772f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "PG_DEFAULT_CONFIG = merge_config(dict(\n",
    "    normalize_advantage=True,\n",
    "\n",
    "    clip_norm=10.0,\n",
    "    clip_gradient=True,\n",
    "\n",
    "    hidden_units=100,\n",
    "\n",
    "    max_iteration=1000,\n",
    "\n",
    "    train_batch_size=1000,\n",
    "    gamma=0.99,\n",
    "    learning_rate=0.001,\n",
    "\n",
    "    env_name=\"CartPole-v1\",\n",
    "\n",
    "), DEFAULT_CONFIG)\n",
    "\n",
    "\n",
    "class PGTrainer(AbstractTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = merge_config(config, PG_DEFAULT_CONFIG)\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.iteration = 0\n",
    "        self.start_time = time.time()\n",
    "        self.iteration_time = self.start_time\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "\n",
    "        # build the model\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Build the policy network and related optimizer\"\"\"\n",
    "        # Detect whether you have GPU or not. Remember to call X.to(self.device)\n",
    "        # if necessary.\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # TODO Build the policy network using CategoricalPolicy\n",
    "        # Hint: Remember to pass config[\"hidden_units\"], and set policy network\n",
    "        #  to the device you are using.\n",
    "        self.network = None\n",
    "        pass\n",
    "        self.network = PGNetwork(\n",
    "            self.obs_dim, self.act_dim,\n",
    "            hidden_units=self.config[\"hidden_units\"]\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Build the Adam optimizer.\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def to_tensor(self, array):\n",
    "        \"\"\"Transform a numpy array to a pytorch tensor\"\"\"\n",
    "        return torch.from_numpy(array).type(torch.float32).to(self.device)\n",
    "\n",
    "    def to_array(self, tensor):\n",
    "        \"\"\"Transform a pytorch tensor to a numpy array\"\"\"\n",
    "        ret = tensor.cpu().detach().numpy()\n",
    "        if ret.size == 1:\n",
    "            ret = ret.item()\n",
    "        return ret\n",
    "\n",
    "    def save(self, loc=\"model.pt\"):\n",
    "        torch.save(self.network.state_dict(), loc)\n",
    "\n",
    "    def load(self, loc=\"model.pt\"):\n",
    "        self.network.load_state_dict(torch.load(loc))\n",
    "\n",
    "    def compute_action(self, observation, eps=None):\n",
    "        \"\"\"Compute the action for single observation. eps is useless here.\"\"\"\n",
    "        assert observation.ndim == 1\n",
    "        # TODO: Sample an action from the action distribution given by the policy.\n",
    "        # Hint: The input of policy network is a tensor with the first dimension to the \n",
    "        #  batch dimension. Therefore you need to expand the first dimension of the observation\n",
    "        #  and converte it to a tensor before feeding it to the policy network.\n",
    "        pass\n",
    "\n",
    "        return action\n",
    "\n",
    "    def compute_log_probs(self, observation, action):\n",
    "        \"\"\"Compute the log probabilities of a batch of state-action pair\"\"\"\n",
    "        # TODO: Use the function of the policy network to get log probs.\n",
    "        # Hint: Remember to transform the data into tensor before feeding it into the network.\n",
    "        pass\n",
    "    \n",
    "        return log_probs\n",
    "\n",
    "    def update_network(self, processed_samples):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        advantages = self.to_tensor(processed_samples[\"advantages\"])\n",
    "        flat_obs = np.concatenate(processed_samples[\"obs\"])\n",
    "        flat_act = np.concatenate(processed_samples[\"act\"])\n",
    "\n",
    "        self.network.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        log_probs = self.compute_log_probs(flat_obs, flat_act)\n",
    "\n",
    "        assert log_probs.shape == advantages.shape, \"log_probs shape {} is not \" \\\n",
    "                                                    \"compatible with advantages {}\".format(log_probs.shape,\n",
    "                                                                                           advantages.shape)\n",
    "\n",
    "        # TODO: Compute the policy gradient loss.\n",
    "        loss = None\n",
    "        pass\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.network.parameters(), self.config[\"clip_gradient\"]\n",
    "        )\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.network.eval()\n",
    "\n",
    "        update_info = {\n",
    "            \"policy_loss\": loss.item(),\n",
    "            \"mean_log_prob\": torch.mean(log_probs).item(),\n",
    "            \"mean_advantage\": torch.mean(advantages).item()\n",
    "        }\n",
    "        return update_info\n",
    "\n",
    "    # ===== Training-related functions =====\n",
    "    def collect_samples(self):\n",
    "        \"\"\"Here we define the pipeline to collect sample even though\n",
    "        any specify functions are not implemented yet.\n",
    "        \"\"\"\n",
    "        iter_timesteps = 0\n",
    "        iter_episodes = 0\n",
    "        episode_lens = []\n",
    "        episode_rewards = []\n",
    "        episode_obs_list = []\n",
    "        episode_act_list = []\n",
    "        episode_reward_list = []\n",
    "        success_list = []\n",
    "        while iter_timesteps <= self.config[\"train_batch_size\"]:\n",
    "            obs_list, act_list, reward_list = [], [], []\n",
    "            obs, info = self.env.reset()\n",
    "            steps = 0\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                act = self.compute_action(obs)\n",
    "\n",
    "                next_obs, reward, terminated, truncated, step_info = self.env.step(act)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                obs_list.append(obs)\n",
    "                act_list.append(act)\n",
    "                reward_list.append(reward)\n",
    "\n",
    "                obs = next_obs.copy()\n",
    "                steps += 1\n",
    "                episode_reward += reward\n",
    "                if done or steps > self.config[\"max_episode_length\"]:\n",
    "                    if \"arrive_dest\" in step_info:\n",
    "                        success_list.append(step_info[\"arrive_dest\"])\n",
    "                    break\n",
    "            iter_timesteps += steps\n",
    "            iter_episodes += 1\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lens.append(steps)\n",
    "            episode_obs_list.append(np.array(obs_list, dtype=np.float32))\n",
    "            episode_act_list.append(np.array(act_list, dtype=np.float32))\n",
    "            episode_reward_list.append(np.array(reward_list, dtype=np.float32))\n",
    "\n",
    "        # The return `samples` is a dict that contains several key-value pair.\n",
    "        # The value of each key-value pair is a list storing the data in one episode.\n",
    "        samples = {\n",
    "            \"obs\": episode_obs_list,\n",
    "            \"act\": episode_act_list,\n",
    "            \"reward\": episode_reward_list\n",
    "        }\n",
    "\n",
    "        sample_info = {\n",
    "            \"iter_timesteps\": iter_timesteps,\n",
    "            \"iter_episodes\": iter_episodes,\n",
    "            \"performance\": np.mean(episode_rewards),  # help drawing figures\n",
    "            \"ep_len\": float(np.mean(episode_lens)),\n",
    "            \"ep_ret\": float(np.mean(episode_rewards)),\n",
    "            \"episode_len\": sum(episode_lens),\n",
    "            \"success_rate\": np.mean(success_list)\n",
    "        }\n",
    "        return samples, sample_info\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        \"\"\"Process samples and add advantages in it\"\"\"\n",
    "        values = []\n",
    "        for reward_list in samples[\"reward\"]:\n",
    "            # reward_list contains rewards in one episode\n",
    "            returns = np.zeros_like(reward_list, dtype=np.float32)\n",
    "            Q = 0\n",
    "\n",
    "            # TODO: Scan the reward_list in a reverse order and compute the\n",
    "            # discounted return at each time step. Fill the array `returns`\n",
    "            pass\n",
    "\n",
    "            values.append(returns)\n",
    "\n",
    "        # We call the values advantage here.\n",
    "        advantages = np.concatenate(values)\n",
    "\n",
    "        if self.config[\"normalize_advantage\"]:\n",
    "            # TODO: normalize the advantage so that it's mean is\n",
    "            # almost 0 and the its standard deviation is almost 1.\n",
    "            pass\n",
    "\n",
    "        samples[\"advantages\"] = advantages\n",
    "        return samples, {}\n",
    "\n",
    "    # ===== Training iteration =====\n",
    "    def train(self, iteration=None):\n",
    "        \"\"\"Here we defined the training pipeline using the abstract\n",
    "        functions.\"\"\"\n",
    "        info = dict(iteration=iteration)\n",
    "\n",
    "        # Collect samples\n",
    "        samples, sample_info = self.collect_samples()\n",
    "        info.update(sample_info)\n",
    "\n",
    "        # Process samples\n",
    "        processed_samples, processed_info = self.process_samples(samples)\n",
    "        info.update(processed_info)\n",
    "\n",
    "        # Update the model\n",
    "        update_info = self.update_network(processed_samples)\n",
    "        info.update(update_info)\n",
    "\n",
    "        now = time.time()\n",
    "        self.iteration += 1\n",
    "        self.total_timesteps += info.pop(\"iter_timesteps\")\n",
    "        self.total_episodes += info.pop(\"iter_episodes\")\n",
    "\n",
    "        # info[\"iter_time\"] = now - self.iteration_time\n",
    "        # info[\"total_time\"] = now - self.start_time\n",
    "        info[\"total_episodes\"] = self.total_episodes\n",
    "        info[\"total_timesteps\"] = self.total_timesteps\n",
    "        self.iteration_time = now\n",
    "\n",
    "        # print(\"INFO: \", info)\n",
    "\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009bfc7",
   "metadata": {},
   "source": [
    "### Section 4.2: Test REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe25068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Test advantage computing\n",
    "test_trainer = PGTrainer({\"normalize_advantage\": False})\n",
    "test_trainer.train()\n",
    "fake_sample = {\"reward\": [[2, 2, 2, 2, 2]]}\n",
    "np.testing.assert_almost_equal(\n",
    "    test_trainer.process_samples(fake_sample)[0][\"reward\"][0],\n",
    "    fake_sample[\"reward\"][0]\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    test_trainer.process_samples(fake_sample)[0][\"advantages\"],\n",
    "    np.array([9.80199, 7.880798, 5.9402, 3.98, 2.], dtype=np.float32)\n",
    ")\n",
    "\n",
    "# Test advantage normalization\n",
    "test_trainer = PGTrainer(\n",
    "    {\"normalize_advantage\": True, \"env_name\": \"CartPole-v1\"})\n",
    "test_adv = test_trainer.process_samples(fake_sample)[0][\"advantages\"]\n",
    "np.testing.assert_almost_equal(test_adv.mean(), 0.0)\n",
    "np.testing.assert_almost_equal(test_adv.std(), 1.0)\n",
    "\n",
    "# Test the shape of functions' returns\n",
    "fake_observation = np.array([\n",
    "    test_trainer.env.observation_space.sample() for i in range(10)\n",
    "])\n",
    "fake_action = np.array([\n",
    "    test_trainer.env.action_space.sample() for i in range(10)\n",
    "])\n",
    "assert test_trainer.to_tensor(fake_observation).shape == torch.Size([10, 4])\n",
    "assert np.array(test_trainer.compute_action(fake_observation[0])).shape == ()\n",
    "assert test_trainer.compute_log_probs(fake_observation, fake_action).shape == \\\n",
    "       torch.Size([10])\n",
    "\n",
    "print(\"Test Passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bee50d",
   "metadata": {},
   "source": [
    "### Section 4.3: Train REINFORCE in CartPole and see the impact of advantage normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6095d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_no_na, pg_result_no_na = run(PGTrainer, dict(\n",
    "    learning_rate=0.001,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "    env_name=\"CartPole-v1\",\n",
    "    normalize_advantage=False,  # <<== Here!\n",
    "\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "), 195.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75805b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_with_na, pg_result_with_na = run(PGTrainer, dict(\n",
    "    learning_rate=0.001,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "    env_name=\"CartPole-v1\",\n",
    "    normalize_advantage=True,  # <<== Here!\n",
    "\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "), 195.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_no_na_df = pd.DataFrame(pg_result_no_na)\n",
    "pg_result_with_na_df = pd.DataFrame(pg_result_with_na)\n",
    "pg_result_no_na_df[\"normalize_advantage\"] = False\n",
    "pg_result_with_na_df[\"normalize_advantage\"] = True\n",
    "\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\",\n",
    "    y=\"performance\",\n",
    "    data=pd.concat([pg_result_no_na_df, pg_result_with_na_df]).reset_index(), hue=\"normalize_advantage\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient: Advantage normalization matters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0d7cc",
   "metadata": {},
   "source": [
    "### Section 4.4: Train REINFORCE in MetaDrive-Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06a73d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "env_name = \"MetaDrive-Tut-Easy-v0\"\n",
    "\n",
    "pg_trainer_metadrive_easy, pg_trainer_metadrive_easy_result = run(PGTrainer, dict(\n",
    "    train_batch_size=2000,\n",
    "    normalize_advantage=True,\n",
    "    max_episode_length=200,\n",
    "    max_iteration=5000,\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "    learning_rate=0.001,\n",
    "    clip_norm=10.0,\n",
    "    env_name=env_name\n",
    "), reward_threshold=120)\n",
    "\n",
    "pg_trainer_metadrive_easy.save(\"pg_trainer_metadrive_easy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b650945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Render the learned behavior\n",
    "# NOTE: The learned agent is marked by green color.\n",
    "eval_reward, eval_info = evaluate(\n",
    "    policy=pg_trainer_metadrive_easy.policy,\n",
    "    num_episodes=1,\n",
    "    env_name=pg_trainer_metadrive_easy.env_name,\n",
    "    render=\"topdown\",  # Visualize the behaviors in top-down view\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "frames = [pygame.surfarray.array3d(f).swapaxes(0, 1) for f in eval_info[\"frames\"]]\n",
    "\n",
    "animate(frames)\n",
    "\n",
    "print(\"REINFORCE agent achieves {} return in MetaDrive easy environment.\".format(eval_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0bcd0",
   "metadata": {},
   "source": [
    "## Section 5: Policy gradient with baseline\n",
    "\n",
    "(20 / 100 points)\n",
    "\n",
    "We compute the gradient of $Q = \\mathop{\\mathbb E} \\sum_t r(a_t, s_t)$ w.r.t. the parameter to update the policy. Let's consider this case: when you take a so-so action that lead to positive expected return, the policy gradient is also positive and you will update your network toward this action. At the same time a potential better action is ignored.\n",
    "\n",
    "To tackle this problem, we introduce the \"baseline\" when computing the policy gradient. The insight behind this is that we want to optimize the policy toward an action that are better than the \"average action\".\n",
    "\n",
    "We introduce $b_{t} = \\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}$ as the baseline. It average the expected discount return of all possible actions at state $s_t$. So that the \"advantage\" achieved by action $a_t$ can be evaluated via $\\sum_{t'=t} \\gamma^{t' -t}r(a_{t'}, s_{t'}) - b_t$\n",
    "\n",
    "Therefore, the policy gradient becomes:\n",
    "\n",
    "$$\\nabla_\\theta Q =\\cfrac{1}{N}\\sum_{i=1}^N [( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) (\\sum_{t'} \\gamma^{t'-t} r(s_{i,{t}}, a_{i,t}) - b_{i, t})]$$\n",
    "\n",
    "In our implementation, we estimate the baseline via an extra network `self.baseline`, which has same structure of policy network but output only a scalar value. We use the output of this network to serve as the baseline, while this network is updated by fitting the true value of expected return of current state: $\\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}$\n",
    "\n",
    "The state-action values might have large variance if the reward function has large variance. It is not easy for a neural network to predict targets with large variance and extreme values. In implementation, we use a trick to match the distribution of baseline and values. During training, we first collect a batch of target values: $\\{t_i= \\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}\\}_i$. Then we normalize all targets to a standard distribution with mean = 0 and std = 1. Then we ask the baseline network to fit such normalized targets.\n",
    "\n",
    "When computing the advantages, instead of using the output of baseline network as the baseline $b$, we firstly match the baseline distribution with state-action values, that is we \"de-standarize\" the baselines. The transformed baselines $b' = f(b)$ should has the same mean and STD with the action values. \n",
    "\n",
    "After that, we compute the advantage of current action: $adv_{i,t} = \\sum_{t'} \\gamma^{t'-t} r(s_{i,{t'}}, a_{i,t'}) - b'_{i, t}$\n",
    "\n",
    "By doing this, we mitigate the instability of training baseline.\n",
    "\n",
    "Hint: We suggest to normalize an array via: `(x - x.mean()) / max(x.std(), 1e-6)`. The max term can mitigate numeraical instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c1378f",
   "metadata": {},
   "source": [
    "### Section 5.1: Build PG method with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709df140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientWithBaselineTrainer(PGTrainer):\n",
    "    def initialize_parameters(self):\n",
    "        # Build the actor in name of self.policy\n",
    "        super().initialize_parameters()\n",
    "\n",
    "        # TODO: Build the baseline network using PytorchModel class.\n",
    "        self.baseline = None\n",
    "        pass\n",
    "\n",
    "        self.baseline_loss = nn.MSELoss()\n",
    "\n",
    "        self.baseline_optimizer = torch.optim.Adam(\n",
    "            self.baseline.parameters(),\n",
    "            lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        # Call the original process_samples function to get advantages\n",
    "        tmp_samples, _ = super().process_samples(samples)\n",
    "        values = tmp_samples[\"advantages\"]\n",
    "        samples[\"values\"] = values  # We add q_values into samples\n",
    "\n",
    "        # Flatten the observations in all trajectories (still a numpy array)\n",
    "        obs = np.concatenate(samples[\"obs\"])\n",
    "\n",
    "        assert obs.ndim == 2\n",
    "        assert obs.shape[1] == self.obs_dim\n",
    "\n",
    "        obs = self.to_tensor(obs)\n",
    "        samples[\"flat_obs\"] = obs\n",
    "\n",
    "        # TODO: Compute the baseline by feeding observation to baseline network\n",
    "        # Hint: baselines turns out to be a numpy array with the same shape of `values`,\n",
    "        #  that is: (batch size, )\n",
    "        baselines = None\n",
    "        pass\n",
    "\n",
    "        assert baselines.shape == values.shape\n",
    "\n",
    "        # TODO: Match the distribution of baselines to the values.\n",
    "        # Hint: We expect to see baselines.std almost equals to values.std, \n",
    "        #  and baselines.mean almost equals to values.mean.\n",
    "        pass\n",
    "\n",
    "        # Compute the advantage\n",
    "        advantages = values - baselines\n",
    "        samples[\"advantages\"] = advantages\n",
    "        process_info = {\"mean_baseline\": float(np.mean(baselines))}\n",
    "        return samples, process_info\n",
    "\n",
    "    def update_network(self, processed_samples):\n",
    "        update_info = super().update_network(processed_samples)\n",
    "        update_info.update(self.update_baseline(processed_samples))\n",
    "        return update_info\n",
    "\n",
    "    def update_baseline(self, processed_samples):\n",
    "        self.baseline.train()\n",
    "        obs = processed_samples[\"flat_obs\"]\n",
    "\n",
    "        # TODO: Normalize `values` to have mean=0, std=1.\n",
    "        values = processed_samples[\"values\"]\n",
    "        pass\n",
    "\n",
    "        values = self.to_tensor(values[:, np.newaxis])\n",
    "\n",
    "        baselines = self.baseline(obs)\n",
    "\n",
    "        self.baseline_optimizer.zero_grad()\n",
    "        loss = self.baseline_loss(input=baselines, target=values)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.baseline.parameters(), self.config[\"clip_gradient\"]\n",
    "        )\n",
    "\n",
    "        self.baseline_optimizer.step()\n",
    "        self.baseline.eval()\n",
    "        return dict(baseline_loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168a4b4",
   "metadata": {},
   "source": [
    "### Section 5.2: Run PG w/ baseline in CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c1adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_wb_cartpole, pg_trainer_wb_cartpole_result = run(PolicyGradientWithBaselineTrainer, dict(\n",
    "    learning_rate=0.001,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "\n",
    "    env_name=\"CartPole-v1\",\n",
    "    normalize_advantage=True,\n",
    "\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10,\n",
    "), 195.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff92a1e",
   "metadata": {},
   "source": [
    "### Section 5.3: Run PG w/ baseline in MetaDrive-Easy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e498197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "env_name = \"MetaDrive-Tut-Easy-v0\"\n",
    "\n",
    "pg_trainer_wb_metadrive_easy, pg_trainer_wb_metadrive_easy_result = run(\n",
    "    PolicyGradientWithBaselineTrainer,\n",
    "    dict(\n",
    "        train_batch_size=2000,\n",
    "        normalize_advantage=True,\n",
    "        max_episode_length=200,\n",
    "        max_iteration=5000,\n",
    "        evaluate_interval=10,\n",
    "        evaluate_num_episodes=10,\n",
    "        learning_rate=0.001,\n",
    "        clip_norm=10.0,\n",
    "        env_name=env_name\n",
    "    ),\n",
    "    reward_threshold=120\n",
    ")\n",
    "\n",
    "pg_trainer_wb_metadrive_easy.save(\"pg_trainer_wb_metadrive_easy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f356062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Render the learned behavior\n",
    "# NOTE: The learned agent is marked by green color.\n",
    "eval_reward, eval_info = evaluate(\n",
    "    policy=pg_trainer_wb_metadrive_easy.policy,\n",
    "    num_episodes=1,\n",
    "    env_name=pg_trainer_wb_metadrive_easy.env_name,\n",
    "    render=\"topdown\",  # Visualize the behaviors in top-down view\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "frames = [pygame.surfarray.array3d(f).swapaxes(0, 1) for f in eval_info[\"frames\"]]\n",
    "\n",
    "print(\n",
    "    \"PG agent achieves {} return and {} success rate in MetaDrive easy environment.\".format(\n",
    "        eval_reward, eval_info[\"success_rate\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "animate(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f267e",
   "metadata": {},
   "source": [
    "### Section 5.4: Run PG with baseline in MetaDrive-Hard\n",
    "\n",
    "**The minimum goal to is to achieve episodic return > 20, which costs nearly 20 iterations and ~100k steps.**\n",
    "\n",
    "## Bonus\n",
    "\n",
    "**BONUS can be earned if you can improve the training performance by adjusting hyper-parameters and optimizing code. Improvement means achieving > 0.0 success rate. However, I can't guarentee it is feasible to solve this task with PG via simplying tweaking the hyper-parameters more carefully.** Please creates a independent markdown cell to highlight your improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737c345",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "env_name = \"MetaDrive-Tut-Hard-v0\"\n",
    "\n",
    "pg_trainer_wb_metadrive_hard, pg_trainer_wb_metadrive_hard_result = run(\n",
    "    PolicyGradientWithBaselineTrainer,\n",
    "    dict(\n",
    "        train_batch_size=4000,\n",
    "        normalize_advantage=True,\n",
    "        max_episode_length=1000,\n",
    "        max_iteration=5000,\n",
    "        evaluate_interval=5,\n",
    "        evaluate_num_episodes=10,\n",
    "        learning_rate=0.001,\n",
    "        clip_norm=10.0,\n",
    "        env_name=env_name\n",
    "    ),\n",
    "    reward_threshold=20  # We just set the reward threshold to 20. Feel free to adjust it.\n",
    ")\n",
    "\n",
    "pg_trainer_wb_metadrive_hard.save(\"pg_trainer_wb_metadrive_hard.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e015832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Render the learned behavior\n",
    "# NOTE: The learned agent is marked by green color.\n",
    "eval_reward, eval_info = evaluate(\n",
    "    policy=pg_trainer_wb_metadrive_hard.policy,\n",
    "    num_episodes=10,\n",
    "    env_name=pg_trainer_wb_metadrive_hard.env_name,\n",
    "    render=None,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "_, eval_info_render = evaluate(\n",
    "    policy=pg_trainer_wb_metadrive_hard.policy,\n",
    "    num_episodes=1,\n",
    "    env_name=pg_trainer_wb_metadrive_hard.env_name,\n",
    "    render=\"topdown\",  # Visualize the behaviors in top-down view\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "frames = [pygame.surfarray.array3d(f).swapaxes(0, 1) for f in eval_info_render[\"frames\"]]\n",
    "\n",
    "print(\n",
    "    \"PG agent achieves {} return and {} success rate in MetaDrive easy environment.\".format(\n",
    "        eval_reward, eval_info[\"success_rate\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "animate(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff92fb1",
   "metadata": {
    "id": "bff92fb1"
   },
   "source": [
    "------\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this assignment, we learn how to build naive Q learning, Deep Q Network and Policy Gradient methods.\n",
    "\n",
    "Following the submission instruction in the assignment to submit your assignment. Thank you!\n",
    "\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
